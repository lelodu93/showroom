# -*- coding: utf-8 -*-


"""hetic_showroom

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14LkbxAoMf8MYlFu4jnhxCH74VYc7d36L

#SHOWROOMPRIVÉ

Dans ce cahier, nous explorons les méthodes d'analyse de textes à l'aide des données des avis clients de Showroom privé.

##Bibliothèque
"""

import pandas as pd

"""##Chargement des données"""

# Charger le fichier Excel
df = pd.read_excel("/content/showroomprive_reviews_cleaned.xlsx", engine="openpyxl")

# Afficher les premières lignes
print(df.head())

"""##Pré-traitement"""

# Pour éviter les cassure je met les caractères en miniscule
df['avis']=df['avis'].str.lower()
df['avis']

df['avis'] = df['avis'].str.replace(r"[^\x00-\x7F]+", "", regex=True)  # Supprime les caractères non ASCII
df['avis']
df

# Assuming df is your DataFrame

# Convert 'date' column to datetime, handling errors
df['date'] = pd.to_datetime(df['date'], errors='coerce')

# Create 'date_only' column
df['date_only'] = df['date'].dt.date

df['date_only'] = pd.to_datetime(df['date_only'], errors='coerce')

# Display 'date_only' column
df['date_only']

print(df.info())  # Structure du fichier

print(df.columns)  # Liste des colonnes

df = df.drop(columns=['date', 'Unnamed: 3', 'Unnamed: 4']) #Suppression des colonnes non nécessaires

df

# Convert 'score' column to numeric, handling non-numeric values
df['score'] = pd.to_numeric(df['score'], errors='coerce')

df['avis'] = df['avis'].astype(str)

print(df.shape)  # Nombre de lignes et colonnes

print(df.info())  # Structure du fichier

df

"""## NLP

Analyse NLP des avis commentaires de showroomprivé
"""

# Charger les avis traités depuis un fichier excel
df = pd.read_excel('/content/hetic-showroom.xlsx')
df

import pandas as pd #Importation des bibliothèques
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from gensim import corpora
from gensim.models import LdaModel
import re

import nltk

df.info()

"""##Application de l'analyse de sentiment"""

def get_sentiment_by_score(score):
    # Convertir le texte en chaîne de caractères pour éviter les erreurs si le texte n'est pas déjà une chaîne
    analysis = TextBlob(str(score))

    """
    Cette fonction prend un score de 0 à 5 et renvoie un sentiment
    basé sur le score.

    - Score de 0 à 2 : négatif
    - Score de 3 : neutre
    - Score de 4 à 5 : positif
    """
    # Si le score est compris entre 0 et 2, le sentiment est négatif
    if score <= 2:
        return 'négatif'

    # Si le score est égal à 3, le sentiment est neutre
    elif score == 3:
        return 'neutre'

    # Si le score est compris entre 4 et 5, le sentiment est positif
    else:
        return 'positif'

# Exemple d'application sur une colonne 'score' dans un DataFrame
df['sentiment'] = df['score'].apply(get_sentiment_by_score)

# Afficher les résultats
print(df[['score', 'sentiment','date_only','avis']])
df_cleaned = df
print(df_cleaned)

# Appliquer l'analyse de sentiment à chaque avis en utilisant la colonne 'score'
df['Sentiment'] = df['score'].apply(get_sentiment_by_score)
df

# Afficher les premiers résultats
print(df_cleaned.head())

import matplotlib.pyplot as plt

# Visualisation des sentiments
sentiment_counts = df_cleaned['Sentiment'].value_counts()

# Création du graphique en barres
ax = sentiment_counts.plot(kind='bar', color=['green', 'red', 'blue'])

# Ajouter un titre et des labels
plt.title('Répartition des sentiments', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Nombre d\'avis', fontsize=14)

# Ajouter les labels au sommet des barres pour plus de clarté
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5),
                textcoords='offset points')

# Afficher le graphique
plt.show()

df_cleaned

""" La subjectivité est une valeur entre 0 et 1 :

0 (objectif) > Texte factuel, avec peu d'opinion.
1 (subjectif) > Texte très subjectif, basé sur des opinions personnelles.
"""
def get_subjectivity(text):
    analysis = TextBlob(str(text))
    return analysis.sentiment.subjectivity

df_cleaned['Subjectivité'] = df['avis'].apply(get_subjectivity)
get_subjectivity
df_cleaned

#df_cleaned.to_excel("hetic-showroom-sentiment.xlsx", index=False, engine="openpyxl")

"""BERTOPIC

BERTopic est une technique plus moderne qui combine BERT et le clustering pour identifier des thèmes plus pertinents.

##2 Analyse Showroom
"""

df_cleaned

print("Shape of the dataset:", df.shape)
print("Columns in the dataset:", df.columns)

df_cleaned.head()

df_cleaned.isnull()
total_null_values = df.isnull().sum().sum()
print("Total null values in the DataFrame:", total_null_values)

df_cleaned.fillna('', inplace=True)

df_cleaned = df_cleaned.dropna(subset=['Sentiment'])

df_cleaned

df_cleaned.isnull()
total_null_values = df_cleaned.isnull().sum().sum()
print("Total null values in the DataFrame:", total_null_values)

print("Earliest review date:", df_cleaned['date_only'].min())
print("Latest review date:", df_cleaned['date_only'].max())

"""#EDA

Bibliothèque
"""

# data processing
import numpy as np
import pandas as pd

# text processing and NLP
import nltk
from textblob import TextBlob

# gensim for topic modeling
import gensim
from gensim import corpora
from gensim.utils import simple_preprocess
# viz

import pyLDAvis.gensim_models as gensimvis
from wordcloud import WordCloud, STOPWORDS
import seaborn as sns
import matplotlib.pyplot as plt

# misc
import re
import warnings
warnings.filterwarnings("ignore")

pip install pyLDAvis

# Convertir la colonne 'score' en type numérique, en gérant les erreurs
df_cleaned['score'] = pd.to_numeric(df['score'], errors='coerce')

# Remplacer les valeurs non numériques par une valeur par défaut, par exemple 0
df_cleaned['score'] = df_cleaned['score'].fillna(0)
# Définir une palette de couleurs pour chaque colonne
colors = plt.cm.get_cmap('viridis', len(score_counts))  # Ou une autre palette


# Analyse de la distribution des scores
score_counts = df_cleaned.score.value_counts().sort_index()
plt.figure(figsize=(2, ))
sns.barplot(x=score_counts.index, y=score_counts.values)
plt.xlabel('Score')
plt.ylabel('Count')
plt.title('Distribution des Scores')
plt.show()

"""# NLP

"""

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

# sentiment distribution
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='score', hue='Sentiment')
plt.xlabel('Score')
plt.ylabel('Count')
plt.title('Sentiment Distribution for Different Scores')
plt.legend(title='Sentiment')
plt.show()

"""#Word cloud

"""

# add our list of stopwords
stopwords = set(STOPWORDS)
stopwords.update(['excellent', 'très bien', 'recommande', 'time', 'app', 'series', 'phone']) # creating a custom list based on domain knowledge

"""negative_reviews = ' '.join(df_cleaned[df_cleaned['avis'] == 'negative'])

wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords).generate(negative_reviews)

import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS

# ... (votre code existant) ...

# 1. Assurez-vous que les stopwords de NLTK sont téléchargés
nltk.download('stopwords')

# 2. Créez votre ensemble de stopwords
stop_words = set(STOPWORDS)  # commencez avec les stopwords de base de wordcloud

# 3. Ajoutez les stopwords français de NLTK
stop_words.update(stopwords.words('french'))

# 4. Ajoutez vos stopwords personnalisés
stop_words.update(['excellent', 'très bien', 'recommande', 'time', 'app', 'series', 'phone'])

# 5. Créez le wordcloud en utilisant votre ensemble de stopwords
wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words).generate(negative_reviews)

# ... (le reste de votre code) ...
plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Negative Reviews Word Cloud')
plt.show()

"""#Modélisation des topics"""

pip install lda

pip install numpy lda

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# 1. Prétraitement du texte
def preprocess_text(text):
    tokens = [token for token in text.lower().split() if token.isalnum() and token not in stopwords.words('french')]
    return ' '.join(tokens) # Joint les jetons pour reformer du texte

texts = [preprocess_text(text) for text in df_cleaned['avis']] # Applique la fonction à la colonne 'avis'

# 2. Tokenisation et création du dictionnaire
from gensim.utils import simple_preprocess
tokenized_texts = [simple_preprocess(text) for text in texts]

from gensim import corpora
dictionary = corpora.Dictionary(tokenized_texts)
dictionary.filter_extremes(no_below=2, no_above=0.95)  # Filtrer les mots trop rares ou trop fréquents

# 3. Création du corpus
corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_texts]

# 4. Création et entraînement du modèle LDA
from gensim.models import LdaMulticore
lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=5, passes=5, workers=2)

# 5. Affichage des topics
n_top_words = 8
for i, topic_dist in enumerate(lda_model.print_topics()):  # Utiliser print_topics() pour le modèle LdaMulticore
    topic_words = re.findall(r'"(.*?)"', topic_dist[1])  # Extraire les mots du format de sortie
    print('Topic {}: {}'.format(i, ', '.join(topic_words)))

# print topics
topics = lda_model.print_topics(-1)
for idx, topic in topics:
    print(f"Topic {idx}: {topic}")

texts[:4]

pip install pyLDAvis

# visualize topics
import pyLDAvis # Assurez-vous d'importer pyLDAvis
vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)

df_cleaned

#df_cleaned.to_excel("hetic-showroom-5mars.xlsx", index=False, engine="openpyxl")

https://www.kaggle.com/code/darrylljk/netflix-reviews-with-nlp#Topic-modeling

"""### STREAMLIT"""

pip install streamlit

# Titre du tableau de bord
st.title("Analyse des avis clients - Showroomprive")
st.write("Ce tableau de bord permet d'analyser les avis clients collectés sur le Google Play Store.")

# Chargement des données (remplace par ton fichier CSV ou ta base de données)
@st.cache_data  # Cache les données pour améliorer les performances
def load_data():
    return pd.read_excel("/content/hetic-showroom-5mars.xlsx")  # Remplace par ton fichier de données

data = load_data()

# Filtres interactifs
st.sidebar.header("Filtres")
sentiment_filter = st.sidebar.multiselect("Filtrer par sentiment", options=data['Sentiment'].unique(), default=data['Sentiment'].unique())

# Application des filtres
filtered_data = data[data['Sentiment'].isin(sentiment_filter)]

# Affichage des KPI
st.header("Indicateurs clés de performance (KPI)")
col1, col2 = st.columns(2)
with col1:
    st.metric("Nombre total d'avis", len(filtered_data))
with col2:
    st.metric("Pourcentage d'avis positifs", f"{len(filtered_data[filtered_data['Sentiment'] == 'positif']) / len(filtered_data) * 100:.2f}%")

# Graphique des sentiments
st.header("Répartition des sentiments")
sentiment_counts = filtered_data['Sentiment'].value_counts()
fig, ax = plt.subplots()
ax.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)
ax.axis('equal')  # Assure un cercle parfait
st.pyplot(fig)

# Tableau des avis
st.header("Avis filtrés")
st.dataframe(filtered_data[['avis', 'Sentiment', 'date_only', 'score', 'Subjectivité']])

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
